# -*- coding: utf-8 -*-
"""PINN_Problemas_Inversos.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17GXZvcevrnScXRCRkCQ-lvDZ0Lx8OFuk

<font face = "Times New Roman" size = "3" color = "black">
<center>
<h3></h3>
<img src = "https://proleon.com.mx/wp-content/uploads/2022/10/01_TM_ESPECIAL_INNOVACION_EDUCATIVA_UNIVERSIDADES_TEC_DE_MONTERREY.png" width = "250">
<h3><b>Campus Estado de México</h3>
<h3>Análisis numérico para la optimización no-lineal</b><br>MA2008B.601<br><br><b>Tercer Avance del Reto<br><br>Autores</b><br>Daniel Makoszay Castañón - A01750046<br>Santiago Jiménez Pasillas - A01749970<br>Santiago Palavicini Saldívar - A01749103<br>Guillermo Ian Barbosa Martínez - A01747926<br><br><b>Profesores</b><br>Miguel Ángel Arvizu Coyotzi<br>Mario Iván Estrada Delgado<br>José Luis Gómez Muñoz</h3>
<img src = "https://www.researchgate.net/profile/Zhen-Li-105/publication/335990167/figure/fig1/AS:806502679982080@1569296631121/Schematic-of-a-physics-informed-neural-network-PINN-where-the-loss-function-of-PINN.ppm" width = "500">
<h6>Crédito de la imagen: <a href = "https://www.researchgate.net/figure/Schematic-of-a-physics-informed-neural-network-PINN-where-the-loss-function-of-PINN_fig1_335990167"><i>Physics-Informed Neural Network (PINN) Schema</i></a></h6>
</img>
</center>
</font>

<font face = "Times New Roman" size = "3" color = "black">
<div align = "justify">
<h2><b>Introducción</b></h2>
<h4>En este trabajo se busca resolver dos sistemas de ecuaciones diferenciales de valor inicial utilizando redes neuronales físicas informadas (Physics-Informed Neural Networks, PINNs). Estos problemas son planteados en el <i>paper</i> de Rodrigues, J. A. (2024), en el que cita la importancia de las PINNs en los modelos de fenómenos físicos y aprendizaje automático. Dichas redes son usadas por el autor para modelar el crecimiento de tumores cancerígenos mediante dos ecuaciones, que son con las que se trabajará a continuación. Para ello, se hará uso de distintas librerías de <i>Python</i>, además de que se ha construido una aplicación, utilizando el paquete de <i>streamlit</i> para hallar los mejores valores paramétricos de dichas ecuaciones diferenciales a emplear en los dos modelos a trabajar, aunado a la visualización que, junto con los valores finales hallados y su ejecución en este <i>notebook</i>, permitirán obtener resultados de manera más eficiente, didáctica y organizada.</h4><br>

<h2><b>Marco Conceptual</b></h2>
<h4>Como se comentó, en esta entrega se trabajará con dos ecuaciones diferenciales de primer orden. Las redes neuronales informadas por la física (PINNs, por sus siglas en inglés, -<i>Physics Informed Neural Networks</i>-) combinan el aprendizaje profundo con el conocimiento de sistemas dinámicos expresados como ecuaciones diferenciales, lo que permite obtener modelos capaces de generalizar incluso a partir de datos ruidosos y escasos, como suele ocurrir en procesos biológicos reales. Este enfoque ha demostrado ser eficaz para modelar fenómenos complejos como el mencionado, de crecimiento de tumores, pues integra datos empíricos con formulaciones matemáticas, tal como se explora en el artículo <i>Using Physics-Informed Neural Networks (PINNs) for Tumor Cell Growth Modeling</i> (Rodrigues, J.A., 2024). En este proyecto, se utiliza la biblioteca <i>DeepXDE</i>, diseñada específicamente para resolver problemas directos e inversos de ecuaciones diferenciales, siguiendo la lógica de ejemplos como el entrenamiento inverso del sistema de Lorenz, donde las PINNs no solo resuelven ecuaciones, sino que también estiman parámetros desconocidos a partir de observacione (Lu Lu, 2019).<br>
<br>Los dos modelos empleados para representar el crecimiento del volumen de células tumorales en esta entrega se basan en ecuaciones diferenciales del tipo logístico. Por un lado, el modelo de Verhulst utiliza una tasa de crecimiento proporcional a la población actual y a su distancia relativa respecto a la capacidad de carga. La ecuación para el modelo es la siguiente:</h4><br>

$$
\frac{dp}{dt}(t) = k \cdot p(t) \cdot (1 - \frac{p(t)}{C})
$$
<br>

<h4>
donde
</h4>

<ul>
  <li><i>p(t)</i>: tamaño de la población de las células en el tiempo <i>t</i>.</li>
  <li><i>t</i>: tiempo (en días).</li>
  <li><i>k</i>: tasa de crecimiento.</li>
  <li><i>C</i>: capacidad de carga.</li>
</ul>

<h4>
Por otro lado, el modelo de Montroll extiende esta lógica introduciendo un parámetro adicional $\theta$, que permite ajustar el ritmo de saturación del crecimiento. El modelo está definido en la siguiente ecuación diferencial:
</h4><br>

$$
\frac{dp}{dt}(t) = k \cdot p(t) \cdot (1 - (\frac{p(t)}{C})^\theta)
$$
<br>

<h4>
donde
</h4>

<ul>
  <li><i>p(t)</i>: tamaño de la población de las células en el tiempo <i>t</i>.</li>
  <li><i>t</i>: tiempo (en días).</li>
  <li><i>k</i>: tasa de crecimiento.</li>
  <li><i>C</i>: capacidad de carga.</li>
  <li>$\theta$: posición del punto de inflexión de la curva de crecimiento.</li>
</ul><br>

<h4>
Además, como se comentó, se utilizará una <i>app</i> de <i>streamlit</i> construida por los autores de este trabajo para agilizar el hallazgo de los parámetros para ambos modelos. La aplicación puede ser utilizada en el siguiente link: <a href = "https://create-tumorcellgrowth-pinn.streamlit.app/"><i>Physics-Informed Neural Networks (PINNs) for Tumor Cell Growth Modeling</i></a>. A su vez, el repositorio donde se encuentra el programa realizado para construir la <i>app</i> se encuentra en el siguiente enlace: <a href = "https://github.com/MeminBMtz/PINN1-app.git"><i>Streamlit PINNs App GitHub Repository</i></a>. Una vez mencionado lo anterior, y tras ejecutar distintos experimentos en la aplicación, a continuación se muestra el código usado.
</h4>

<font face = "Times New Roman" size = "3" color = "black">
<div align = "justify">
<h2><b>Librerías y dependencias necesarias</b></h2>
"""

pip install deepxde

import deepxde as dde
import numpy as np
import pandas as pd
import plotly.graph_objects as go
from deepxde.backend import tf
from scipy.integrate import solve_ivp
import re

"""<font face = "Times New Roman" size = "3" color = "black">
<div align = "justify">
<h2><b>Entrenamiento, Validación y Comparación (Reales vs. Predicciones vs. Solución Analítica con parámetros) del Modelo de <i>Verhulst</i></b></h2>
"""

# Elegir modelo: "Verhulst" o "Montroll"
modelo = "Verhulst"
lr = 0.001
iters = 5000
params = {'k': 3, 'C': 10, 'theta': 2.5}

def get_data():
    # Datos del tiempo (t) en días
    t = np.array([3.46, 4.58, 5.67, 6.64, 7.63, 8.41, 9.32, 10.27, 11.19, 12.39, 13.42, 15.19, 16.24, 17.23, 18.18, 19.29,
                  21.23, 21.99, 24.33, 25.58, 26.43, 27.44, 28.43, 30.49, 31.34, 32.34, 33.0, 35.2, 36.34, 37.29, 38.5, 39.67,
                  41.37, 42.58, 45.39, 46.38, 48.29, 49.24, 50.19, 51.14, 52.10, 54.0, 56.33, 57.33, 59.38])
    # Volumen de las células cancerígenas 10^9 νm3
    V = np.array([0.0158, 0.0264, 0.0326, 0.0445, 0.0646, 0.0933, 0.1454, 0.2183, 0.2842, 0.4977, 0.6033, 0.8441, 1.2163, 1.447, 2.3298,
                  2.5342, 3.0064, 3.4044, 3.2046, 4.5241, 4.3459, 5.1374, 5.5376, 4.8946, 5.0660, 6.1494, 6.8548, 5.9668, 6.6945, 6.6395,
                  6.8971, 7.2966, 7.2268, 6.8815, 8.0993, 7.2112, 7.0694, 7.4971, 6.9974, 6.7219, 7.0523, 7.1095, 7.0694, 8.0562, 7.2268])
    return pd.DataFrame({'t': t, 'V': V})

# Extracción de datos
df = get_data()

# Ejecución del modelo
def run_model(model="Verhulst", vars=params, lr=0.001, iters=5000):
    # Dominio del problema (días: [3.46, 50.38])
    geom = dde.geometry.TimeDomain(df['t'].min(), df['t'].max())

    # Organización y asignación de los datos de entrenamiento
    observe_t = df['t'].values
    observe_V = df['V'].values
    observe_Ys = dde.icbc.PointSetBC(observe_t.reshape(-1, 1), observe_V.reshape(-1, 1), component=0)

    # Definición de las variables para ambos modelos
    k = dde.Variable(vars['k'])
    C = dde.Variable(vars['C'])

    if model == "Verhulst":
        def verhulst(t, v):
            # Función que representa la ecuación diferencial a resolver, con v (volumen) = p(t) (población de células en tiempo t)
            dpdt = dde.grad.jacobian(v, t, i=0, j=0)
            return dpdt - k * v * (1 - v / C)

        # Arquitectura de la red neuronal: 3 capas ocultas con 50 neuronas cada una y activación tanh
        net = dde.nn.FNN([1] + [50]*3 + [1], "tanh", "Glorot uniform")

        # Definición de los datos del problema y los parámetros a hallar
        data = dde.data.PDE(geom, verhulst, [observe_Ys], num_domain=400, num_boundary=2, anchors=observe_t.reshape(-1, 1))
        variable = dde.callbacks.VariableValue([k, C], period=100, filename="variables.dat")

        # Creación y compilación del modelo con red, datos y optimizador Adam
        model = dde.Model(data, net)
        model.compile("adam", lr=lr, external_trainable_variables=[k, C])
        losshistory, train_state = model.train(iterations=iters, callbacks=[variable])

        # Puntos de prueba en el dominio para comparar la predicción del modelo con la solución exacta
        x_test = geom.uniform_points(45, True)
        y_pred = model.predict(x_test)
        return x_test, y_pred, losshistory, model.sess.run(k), model.sess.run(C), None

    if model == "Montroll":
        # Definición de theta para el Montroll
        theta = dde.Variable(vars['theta'])

        def montroll(t, v):
            # Función que representa la ecuación diferencial a resolver, con v (volumen) = p(t) (población de células en tiempo t)
            dpdt = dde.grad.jacobian(v, t, i=0, j=0)
            v_pos = tf.nn.softplus(v)
            return dpdt - k * v_pos * (1 - (v_pos / C) ** theta)

        # Arquitectura de la red neuronal: 3 capas ocultas con 50 neuronas cada una y activación tanh
        net = dde.nn.FNN([1] + [50]*3 + [1], "tanh", "Glorot uniform")

        # Definición de los datos del problema y los parámetros a hallar
        data = dde.data.PDE(geom, montroll, [observe_Ys], num_domain=400, num_boundary=2, anchors=observe_t.reshape(-1, 1))
        variable = dde.callbacks.VariableValue([k, C, theta], period=100, filename="variables.dat")

        # Creación y compilación del modelo con red, datos y optimizador Adam
        model = dde.Model(data, net)
        model.compile("adam", lr=lr, external_trainable_variables=[k, C, theta])
        losshistory, train_state = model.train(iterations=iters, callbacks=[variable])

        # Puntos de prueba en el dominio para comparar la predicción del modelo con la solución exacta
        x_test = geom.uniform_points(45, True)
        y_pred = model.predict(x_test)
        return x_test, y_pred, losshistory, model.sess.run(k), model.sess.run(C), model.sess.run(theta)

# Obtención del modelo
x_test, y_pred, losshistory, final_k, final_C, final_theta = run_model(modelo, params, lr, iters)


def get_scipy_vals(df, model_eq, vars):
    # SciPy Solution
    # Se define el dominio, y la p(0) inicial
    p0 = [float(df['V'].values[0])]
    t_span = (df['t'].min(), df['t'].max())
    t_eval = np.linspace(t_span[0], t_span[1], 45)

    # Ecuación diferencial de cada modelo
    if model_eq == "Verhulst":
        def eq(t, p): return vars['k'] * p * (1 - p / vars['C'])
    else:
        def eq(t, p): return vars['k'] * p * (1 - (p / vars['C']) ** vars['theta'])

    # Solución de la ecuación en el dominio, con las condiciones y los parámetros obtenidos
    sol = solve_ivp(eq, t_span, p0, t_eval=t_eval)
    return sol.t, sol.y[0]

# Obtención de predicciones con la solución analítica
x_scipy, y_scipy = get_scipy_vals(df, modelo, {**params, 'k': final_k, 'C': final_C, 'theta': final_theta})

# Figura de visualización de datos reales vs. predicciones modelo vs. predicciones de la solución analítica
fig = go.Figure()
fig.add_trace(go.Scatter(x=df['t'], y=df['V'], mode='markers', name="Real Points", marker=dict(color='cyan')))
fig.add_trace(go.Scatter(x=x_test.flatten(), y=y_pred.flatten(), mode='lines', name="PINN Prediction", line=dict(color='blue')))
fig.add_trace(go.Scatter(x=x_scipy.flatten(), y=y_scipy.flatten(), mode='lines', name="Analytical Solution", line=dict(color='white')))
fig.update_layout(template='plotly_dark', title=f"<b>{modelo} Model Prediction</b>", title_x=0.5,
                  xaxis_title='t (days)', yaxis_title='V (tumor cells volume)', height=500, width=1000)
fig.show()

# Suma de los componentes de la pérdida para entrenamiento y validación (Loss train y loss test)
loss_total_train = [np.sum(l) for l in losshistory.loss_train]
loss_total_test = [np.sum(l) for l in losshistory.loss_test]

# Figura de visualización de la función de pérdida del modelo
fig2 = go.Figure()
fig2.add_trace(go.Scatter(y=loss_total_train, mode='lines', name='Train Loss', line=dict(color='red')))
fig2.add_trace(go.Scatter(y=loss_total_test, mode='lines', name='Test Loss', line=dict(color='orange')))
fig2.update_layout(template='plotly_dark', title='<b>Loss Curve</b>', yaxis_type='log',
                   xaxis_title='Epoch', yaxis_title='Loss', height=500, width=1000)
fig2.show()

# Lectura y tratamiento de las variables y su evolución en el entrenamiento
vars_df = pd.read_table("variables.dat", sep="\s+", comment="#", header=None)

# Manipulación de las columnas en función del modelo elegido
if vars_df.shape[1] == 3:
    vars_df.columns = ["epoch", "k", "C"]
elif vars_df.shape[1] == 4:
    vars_df.columns = ["epoch", "k", "C", "theta"]

# Se convierten todos los valores de str y formato científico a float
for col in vars_df.columns:
    vars_df[col] = vars_df[col].astype(str).apply(lambda x: float(re.sub(r"[\[\],]", "", x.strip())))

# Configuración de la visualización de la evolución de la variables 'k' por epochs
fig_k = go.Figure()
fig_k.add_trace(go.Scatter(y=vars_df["k"], mode='lines', name='k', line=dict(color='green')))
fig_k.update_layout(template='plotly_dark', title='<b>k Evolution</b>', xaxis_title='Step', yaxis_title='k')
fig_k.show()

# Configuración de la visualización de la evolución de la variables 'C' por epochs
fig_c = go.Figure()
fig_c.add_trace(go.Scatter(y=vars_df["C"], mode='lines', name='C', line=dict(color='blue')))
fig_c.update_layout(template='plotly_dark', title='<b>C Evolution</b>', xaxis_title='Step', yaxis_title='C')
fig_c.show()

if modelo == "Montroll":
    # Configuración de la visualización de la evolución de la variables 'theta' por epochs
    fig_theta = go.Figure()
    fig_theta.add_trace(go.Scatter(y=vars_df["theta"], mode='lines', name='θ', line=dict(color='purple')))
    fig_theta.update_layout(template='plotly_dark', title='<b>θ Evolution</b>', xaxis_title='Step', yaxis_title='θ')
    fig_theta.show()

# Valores obtenidos para cada una de las variables
print(f"Final k: {final_k:.4f}")
print(f"Final C: {final_C:.4f}")
if final_theta is not None:
    print(f"Final θ: {final_theta:.4f}")

"""<font face = "Times New Roman" size = "3" color = "black">
<div align = "justify">
<h2><b>Entrenamiento, Validación y Comparación (Reales vs. Predicciones vs. Solución Analítica con parámetros) del Modelo de <i>Montroll</i></b></h2>
"""

# Elegir modelo: "Verhulst" o "Montroll"
modelo = "Montroll"
lr = 0.001
iters = 5000
params = {'k': 2.5, 'C': 10, 'theta': 3}

def get_data():
    # Datos del tiempo (t) en días
    t = np.array([3.46, 4.58, 5.67, 6.64, 7.63, 8.41, 9.32, 10.27, 11.19, 12.39, 13.42, 15.19, 16.24, 17.23, 18.18, 19.29,
                  21.23, 21.99, 24.33, 25.58, 26.43, 27.44, 28.43, 30.49, 31.34, 32.34, 33.0, 35.2, 36.34, 37.29, 38.5, 39.67,
                  41.37, 42.58, 45.39, 46.38, 48.29, 49.24, 50.19, 51.14, 52.10, 54.0, 56.33, 57.33, 59.38])

    # Volumen de las células cancerígenas 10^9 νm3
    V = np.array([0.0158, 0.0264, 0.0326, 0.0445, 0.0646, 0.0933, 0.1454, 0.2183, 0.2842, 0.4977, 0.6033, 0.8441, 1.2163, 1.447, 2.3298,
                  2.5342, 3.0064, 3.4044, 3.2046, 4.5241, 4.3459, 5.1374, 5.5376, 4.8946, 5.0660, 6.1494, 6.8548, 5.9668, 6.6945, 6.6395,
                  6.8971, 7.2966, 7.2268, 6.8815, 8.0993, 7.2112, 7.0694, 7.4971, 6.9974, 6.7219, 7.0523, 7.1095, 7.0694, 8.0562, 7.2268])
    return pd.DataFrame({'t': t, 'V': V})

# Extracción de datos
df = get_data()

# Ejecución del modelo
def run_model(model="Verhulst", vars=params, lr=0.001, iters=5000):

    # Dominio del problema (días: [3.46, 50.38])
    geom = dde.geometry.TimeDomain(df['t'].min(), df['t'].max())

    # Organización y asignación de los datos de entrenamiento
    observe_t = df['t'].values
    observe_V = df['V'].values
    observe_Ys = dde.icbc.PointSetBC(observe_t.reshape(-1, 1), observe_V.reshape(-1, 1), component=0)

    # Definición de las variables para ambos modelos
    k = dde.Variable(vars['k'])
    C = dde.Variable(vars['C'])

    if model == "Verhulst":
        def verhulst(t, v):
            # Función que representa la ecuación diferencial a resolver, con v (volumen) = p(t) (población de células en tiempo t)
            dpdt = dde.grad.jacobian(v, t, i=0, j=0)
            return dpdt - k * v * (1 - v / C)

        net = dde.nn.FNN([1] + [50]*3 + [1], "tanh", "Glorot uniform")
        data = dde.data.PDE(geom, verhulst, [observe_Ys], num_domain=400, num_boundary=2, anchors=observe_t.reshape(-1, 1))
        variable = dde.callbacks.VariableValue([k, C], period=100, filename="variables.dat")
        model = dde.Model(data, net)
        model.compile("adam", lr=lr, external_trainable_variables=[k, C])
        losshistory, train_state = model.train(iterations=iters, callbacks=[variable])
        x_test = geom.uniform_points(45, True)
        y_pred = model.predict(x_test)
        return x_test, y_pred, losshistory, model.sess.run(k), model.sess.run(C), None

    if model == "Montroll":
        theta = dde.Variable(vars['theta'])

        def montroll(t, v):
            dpdt = dde.grad.jacobian(v, t, i=0, j=0)
            v_pos = tf.nn.softplus(v)
            return dpdt - k * v_pos * (1 - (v_pos / C) ** theta)

        net = dde.nn.FNN([1] + [50]*3 + [1], "tanh", "Glorot uniform")
        data = dde.data.PDE(geom, montroll, [observe_Ys], num_domain=400, num_boundary=2, anchors=observe_t.reshape(-1, 1))
        variable = dde.callbacks.VariableValue([k, C, theta], period=100, filename="variables.dat")
        model = dde.Model(data, net)
        model.compile("adam", lr=lr, external_trainable_variables=[k, C, theta])
        losshistory, train_state = model.train(iterations=iters, callbacks=[variable])
        x_test = geom.uniform_points(45, True)
        y_pred = model.predict(x_test)
        return x_test, y_pred, losshistory, model.sess.run(k), model.sess.run(C), model.sess.run(theta)

x_test, y_pred, losshistory, final_k, final_C, final_theta = run_model(modelo, params, lr, iters)

def get_scipy_vals(df, model_eq, vars):
    p0 = [float(df['V'].values[0])]
    t_span = (df['t'].min(), df['t'].max())
    t_eval = np.linspace(t_span[0], t_span[1], 45)

    if model_eq == "Verhulst":
        def eq(t, p): return vars['k'] * p * (1 - p / vars['C'])
    else:
        def eq(t, p): return vars['k'] * p * (1 - (p / vars['C']) ** vars['theta'])

    sol = solve_ivp(eq, t_span, p0, t_eval=t_eval)
    return sol.t, sol.y[0]

x_scipy, y_scipy = get_scipy_vals(df, modelo, {**params, 'k': final_k, 'C': final_C, 'theta': final_theta})

fig = go.Figure()
fig.add_trace(go.Scatter(x=df['t'], y=df['V'], mode='markers', name="Real Points", marker=dict(color='cyan')))
fig.add_trace(go.Scatter(x=x_test.flatten(), y=y_pred.flatten(), mode='lines', name="PINN Prediction", line=dict(color='blue')))
fig.add_trace(go.Scatter(x=x_scipy.flatten(), y=y_scipy.flatten(), mode='lines', name="Analytical Solution", line=dict(color='white')))
fig.update_layout(template='plotly_dark', title=f"<b>{modelo} Model Prediction</b>", title_x=0.5,
                  xaxis_title='t (days)', yaxis_title='V (tumor cells volume)', height=500, width=1000)
fig.show()

loss_total_train = [np.sum(l) for l in losshistory.loss_train]
loss_total_test = [np.sum(l) for l in losshistory.loss_test]
fig2 = go.Figure()
fig2.add_trace(go.Scatter(y=loss_total_train, mode='lines', name='Train Loss', line=dict(color='red')))
fig2.add_trace(go.Scatter(y=loss_total_test, mode='lines', name='Test Loss', line=dict(color='orange')))
fig2.update_layout(template='plotly_dark', title='<b>Loss Curve</b>', yaxis_type='log',
                   xaxis_title='Epoch', yaxis_title='Loss', height=500, width=1000)
fig2.show()

vars_df = pd.read_table("variables.dat", sep="\s+", comment="#", header=None)

if vars_df.shape[1] == 3:
    vars_df.columns = ["epoch", "k", "C"]
elif vars_df.shape[1] == 4:
    vars_df.columns = ["epoch", "k", "C", "theta"]

for col in vars_df.columns:
    vars_df[col] = vars_df[col].astype(str).apply(lambda x: float(re.sub(r"[\[\],]", "", x.strip())))

fig_k = go.Figure()
fig_k.add_trace(go.Scatter(y=vars_df["k"], mode='lines', name='k', line=dict(color='green')))
fig_k.update_layout(template='plotly_dark', title='<b>k Evolution</b>', xaxis_title='Step', yaxis_title='k')
fig_k.show()

fig_c = go.Figure()
fig_c.add_trace(go.Scatter(y=vars_df["C"], mode='lines', name='C', line=dict(color='blue')))
fig_c.update_layout(template='plotly_dark', title='<b>C Evolution</b>', xaxis_title='Step', yaxis_title='C')
fig_c.show()

if modelo == "Montroll":
    fig_theta = go.Figure()
    fig_theta.add_trace(go.Scatter(y=vars_df["theta"], mode='lines', name='θ', line=dict(color='purple')))
    fig_theta.update_layout(template='plotly_dark', title='<b>θ Evolution</b>', xaxis_title='Step', yaxis_title='θ')
    fig_theta.show()

print(f"Final k: {final_k:.4f}")
print(f"Final C: {final_C:.4f}")
if final_theta is not None:
    print(f"Final θ: {final_theta:.4f}")

"""<font face = "Times New Roman" size = "3" color = "black">
<div align = "justify">
<h2><b>Análisis</b></h2>
<h4>En ambos modelos, la red neuronal fue capaz de ajustarse adecuadamente a los datos observacionales, mostrando una buena concordancia entre la predicción de la PINN y la solución analítica construida con los parámetros aprendidos. En el modelo de Verhulst, los valores hallados para $k=0.2949$ y $C=6.9702$ reflejan un crecimiento sigmoidal bien limitado. Por otro lado, en el modelo de Montroll, la red capturó una dinámica más compleja, obteniendo $k=1.1345$, $C=7.3683$ y un valor bajo de $\theta=0.1032$ que sugiere un crecimiento casi exponencial en su etapa inicial. Las curvas de pérdida en ambos casos muestran convergencia estable y sin signos evidentes de sobreajuste.</h4><br>

<h2><b>Conclusión</b></h2>
<h4>Los modelos implementados con PINNs demostraron ser efectivos para ajustar datos reales de crecimiento tumoral, tanto en su versión logística básica (Verhulst) como en la versión no lineal generalizada (Montroll). La capacidad de estimar parámetros directamente desde los datos, junto con la estabilidad observada en el entrenamiento y validación, respalda el potencial de las PINNs como herramienta precisa y flexible en problemas de modelado biológico.
</h4><br>

<h2><b>Referencias</b></h2>
<h4>
<ol>
<li>
<i>Inverse problem for the Lorenz system — DeepXDE 1.14.1.dev1+ge99a95b documentation. (s/f). Readthedocs.Io</i>. Recuperado de <a href = "https://deepxde.readthedocs.io/en/latest/demos/pinn_inverse/lorenz.inverse.html"><i>Inverse problem for the Lorenz system</i></a>
</li>
<li>
No title. (s/f). Streamlit.App. Recuperado de <a href = "https://create-tumorcellgrowth-pinn.streamlit.app/"><i>Physics-Informed Neural Networks (PINNs) for Tumor Cell Growth Modeling</i></a>
</li>
<li>
Rodrigues, J. A. (2024). Mdpi.com. Recuperado de <a href = "https://www.mdpi.com/2227-7390/12/8/1195"><i>Using Physics-Informed Neural Networks (PINNs) for Tumor Cell Growth Modeling</i></a>
</li>
</ol>
</h4>
"""

